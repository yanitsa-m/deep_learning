{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "- Applying artifical constraints on the network that implicitly reduce the number of free parameters while not making it more difficult to optimize.\n",
    "\n",
    "-  Using **L2 Regularization**, we add another term to loss that penalizes large weights - this is typically achieved by adding the L2 norm of the weights to the loss multiplied by a small constant.\n",
    "-  The **L2 norm** is the sum of the squares of the individual elements in a vector.\n",
    "-  Another technique for regularization is **Dropout**, where we take a random subset of the activations values (being passed between two layers) and set half of them to 0. Essentially, we take half of the data flowing through network and destroy it. \n",
    "-  Using dropout, the network can never rely on any given activation to be present, so it is forced to learn a redundant represention for everything to make sure at least some of the information remains. This prevents overfitting and improves performance - the network takes a consesus over an ensemble of networks.\n",
    "-  When evaluating a network that's been trained with dropout, we do not want the randomness. Instead, we take the consensus over the redundant models. To get the consensus, average the activations. During training, zero out activations and scale the remaining ones by a factor of 2. Duringe evaluation remove scaling factor and dropouts.\n",
    "-  If dropout does not work, use a bigger network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply dropout to a neural network\n",
    "\n",
    "-  The `tf.nn.dropout()` function takes in two parameters:\n",
    "\n",
    "    `hidden_layer`: the tensor to which you would like to apply dropout <br />\n",
    "    `keep_prob`: the probability of keeping (i.e. not dropping) any given unit\n",
    "    \n",
    "\n",
    "-  `keep_prob` allows you to adjust the number of units to drop. In order to compensate for dropped units, `tf.nn.dropout()` multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.\n",
    "\n",
    "-  During training, a good starting value for keep_prob is 0.5.\n",
    "\n",
    "-  During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.45999908   9.39999866]\n",
      " [  0.11200001   0.67200011]\n",
      " [ 43.30000305  48.15999985]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits, feed_dict={keep_prob: 0.5}))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
