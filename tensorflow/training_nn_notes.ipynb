{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss and Gradient descent\n",
    "\n",
    "- The **training loss** is the average cross entropy over the entire training set, in other words it measures the average of the distances between the inputs and the labels for the training set.\n",
    "-  We hope to find the values for the weights and biases such that we get a high distance when a traning example is misclassified, and low distance when the correct label is predicted.\n",
    "-  Therefore, we want small distances and overall a small value for the training loss of our classifier. That means it is doing a good job of classifying the data based on the target labels.\n",
    "-  The loss is a function of the weights and biases so we seek to minimize the function for the loss.\n",
    "-  **Gradient descent** is a method for doing numerical optimization in machine learning.\n",
    "-  We can take the derivative of the loss with respect to its parameters, then follow that derivative by taking a step backwards until we reach the minimum of the loss.\n",
    "-  **Stochastic gradient descent** is a more efficient method and it computes the average loss for a *small random fraction* of the dataset. Therefore instead of giving an exact measure, it provides an estimate of the loss. It speeds up computation at each step by computing less derivatives, but takes many more \"smaller noisier steps\" or iterations until convergence. The gradient descent might at times even take steps in the wrong direction and increase the real loss not reduce it! However SGD scales well with data and model size and is used for deep learning.\n",
    "-  The *momentum* technique keeps a running average of the gradients and uses that instead of the direction of the current batch of data. It allows for better convergence.\n",
    "-  The *learning rate decay* means that the gradient descent takes smaller and smaller steps as learning progresses (ex: exponential decay).\n",
    "-  It is important to use zero mean & equal variance inputs and weights initialized to small random values & with equal variance for SGD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
