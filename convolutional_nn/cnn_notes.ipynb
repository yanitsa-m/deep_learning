{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "-  Broadly speaking, the CNN learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image as we progress through the layers of the network. Finally, it classifies the image by combining the larger, more complex objects. \n",
    "-  CNN share parameters across space.\n",
    "-  An image has width, height, and depth (# of color channels).\n",
    "-  CNNs take out a 'patch' or **kernel** of the image moving across the whole image and create a mini network (with fewer weights that are shared across space). Instead having layers of matrices to multiply, we have stacks of convolutions.\n",
    "-  They will form a pyramid - at the bottom it is a big shallow image of RGB, then progressively squeeze the spatial dimensions at each layer while increasing the depth. At the top the spatial information is squeezed out but the parameters that map the content to the image remain.\n",
    "-  Each layer of the pyramid or stack is called a **feature map**.\n",
    "-  The **stride** is the number of pixels that you are shifting when moving the window or filter across the image.\n",
    "-  Valid padding - remain withing image borders/edges or **same padding** - go off the edge of the image and pad with zeroes such that output map size is exactly the same as input map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking up an Image\n",
    "\n",
    "-  The first step for a CNN is to break up the image into smaller pieces. We do this by selecting a width and height that defines a filter.\n",
    "-  The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter.\n",
    "-  We then simply slide this filter horizontally or vertically to focus on a different piece of the image.\n",
    "-  The amount by which the filter slides is referred to as the 'stride'. The stride is a hyperparameter which can be tuned. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\n",
    "-  The key idea is that we are *grouping together adjacent pixels* and treating them as a collective because  pixels in an image are close together for a reason and have special meaning. \n",
    "\n",
    "#### Filter depth\n",
    "\n",
    "- It's common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a convolutional layer is called the filter depth.\n",
    "-  How many neurons does each patch connect to?\n",
    "-  That’s dependent on our filter depth. If we have a depth of k, we connect each patch of pixels to k neurons in the next layer. This gives us the height of k in the next layer, as shown below. In practice, k is a hyperparameter we tune, and most CNNs tend to pick the same starting values.\n",
    "\n",
    "#### Parameter Sharing\n",
    "\n",
    "-  Translational invariance - we don't care where the object is located in the image.\n",
    "-  To classify an object, we have to use the same weights and biases for objects of the same type regardless of where they are in the image so that they are both classified as the same type.\n",
    "-  This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "\n",
    "#### Padding\n",
    "-  We keep the same dimensions between layers by adding zeroes.\n",
    "-  TensorFlow uses the following equation for 'SAME' vs 'VALID'\n",
    "\n",
    "SAME Padding, the output height and width are computed as:\n",
    "\n",
    "out_height = ceil(float(in_height) / float(strides[1]))\n",
    "\n",
    "out_width = ceil(float(in_width) / float(strides[2]))\n",
    "\n",
    "VALID Padding, the output height and width are computed as:\n",
    "\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "\n",
    "out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "\n",
    "#### Dimensionality \n",
    "\n",
    "-  From what we've learned so far, how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "-  Given:\n",
    "\n",
    "    our input layer has a width of W and a height of H <br />\n",
    "    our convolutional layer has a filter size F<br />\n",
    "    we have a stride of S<br />\n",
    "    a padding of P<br />\n",
    "    and the number of filters K,<br />\n",
    "\n",
    "\n",
    "-  the following formula gives us the width of the next layer: W_out =[ (W−F+2P)/S] + 1.\n",
    "-  The output height would be H_out = [(H-F+2P)/S] + 1.\n",
    "-  And the output depth would be equal to the number of filters D_out = K.\n",
    "-  The output volume would be W_out * H_out * D_out.\n",
    "-  Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
